---
layout: post
comments: true
title:  "Gradients - Part 4"
excerpt: "Part 4 of computing gradients for training Neural Nets"
date:   2016-09-24 15:40:00
mathjax: true
---


### **2 layer network, single training example (vector)**

In this post we will consider 2 type of networks. In first network, hidden layer has only 1 node while in second network hideen layer has more than 1 node. Lets start with the case of hidden layer with only 1 node.


<div class="imgcap">
<img src="/assets/gradients/NN_4_1.jpeg" height="300" width="350">
<div class="thecap">Neural net with 2 layer, 1 node in hidden layer. Input is a vector</div>
</div>

First layer (\\(l_1\\)) has weight matrix $$[W_1]_{\scriptscriptstyle 3 \times 1}$$
$$
\begin{equation}
     W_1=\begin{bmatrix}
         w_{1}^{1} \\
         w_{1}^{2} \\
         w_{1}^{3} \\
         \end{bmatrix}
\end{equation}
$$

Second layer (\\(l_2\\)) has weight matrix $$[W_2]_{\scriptscriptstyle 1 \times 1}$$ which is a scalar 

$$
\begin{equation}
     W_2=\begin{bmatrix}
         w_{2} \\
         \end{bmatrix}
\end{equation}
$$

##### **Input & Output definitions**

Input is \\((\vec{X},y)\\) : \\(\vec{X}\\) is a vector, while y is a scalar. 

\\(X = [x^1 ~~x^2 ~~x^3]\\)     &nbsp; &nbsp; &nbsp; \\(x^i = i^{th}\\) component of \\(\vec{X}\\).

Thus, in matrix form x,y are $$[X]_{\scriptscriptstyle 1\times 3}$$ and $$[y]_{\scriptscriptstyle 1\times 1}$$.

Let \\( l_1 \\) be output of layer 1. In matrix format, \\([l_1]_{\scriptscriptstyle 1\times 1}\\)
\\( l_1 = \sigma ([X] . [W_1]) = \frac{1}{1 + e^{-[X] . [W_1]}} \label{ref10} \tag{1} \\)

$$
\begin{align}
l_1 & = \sigma ([X] . [W_1]) \\
& = \frac{1}{1 + e^{-[X] . [W_1]}} \\
& = \frac{1}{1 + e^{-(x^1 \times w_1^1 + x^2 \times w_1^2 + x^3 \times w_1^3)}}
\end{align}
$$


\\( \hat{y} \\) is predicted output. In matrix format, \\([\hat{y}]_{\scriptscriptstyle 1\times 1}\\)
\\( \hat{y} = \sigma ([l_1] . [W_2]) = \frac{1}{1 + e^{-[l_1] . [W_2]}} \label{ref11} \tag{2} \\)


##### **Loss**
Like before, we will use half of squared error loss. $$ L  = \frac{1}{2} (y - \hat{y})^{2} $$



##### **Gradients**

There are 2 set of gradients: \\(\nabla_{W_1} L\\) and \\(\nabla_{W_2} L\\). Let us first compute \\(\nabla_{W_2} L\\)


**\\([\nabla_{W_2} L]_{\scriptscriptstyle 1\times 1}\\)**

$$
\begin{align}
\frac{\partial L}{\partial W_2} & = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial W_2} \label{ref12} \tag{3}\\
\frac{\partial L}{\partial \hat{y}} &= \frac{1}{2} \times 2 \times (y - \hat{y})^{1} \times (-1) \label{ref13} \tag{4}\\
\frac{\partial \hat{y}}{\partial W_2} &= \big{(} \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} \times \big{(}1- \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} * -l_1 \\
& = \sigma ([l_1] . [W_2]) \times (1- \sigma ([l_1] . [W_2])) * -l_1 \dots && \text{using \eqref{ref11}} \label{ref14} \tag{5}\\
& = \hat{y} \times (1- \hat{y}) * -l_1 \dots && \text{using \eqref{ref11}} \label{ref15} \tag{6}\\
\end{align}
$$

Substituting \eqref{ref13} & \eqref{ref15} in \eqref{ref12}, we get 

$$
\begin{align}
\frac{\partial L}{\partial W_2} &= \big{(} (-1) \times (y - \hat{y}) \big{)} \times \big{(} \hat{y} \times (1- \hat{y}) \times -l_1 \big{)}\\ \\
&= (y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \times l_1 \label{ref16} \tag{7} \\
\end{align}
$$

Let,  

\begin{align}
\Delta l_{2} = (y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \label{ref17} \tag{8} \\
\end{align}

Then, eq \eqref{ref16} reduces to:   
$$ 
\begin{align}
\frac{\partial L}{\partial W} &= \Delta l_{2} \times l_1 \\
& = \Delta l_{2} * l_1 \\
& = [l_1^{T}] . \Delta l_{2} \label{ref18} \tag{9} \\
\end{align}
$$

**\\([\nabla_{W_1} L]_{\scriptscriptstyle 3\times 1}\\)**

Now let us compute \\(\nabla_{W_1} L\\)

$$
\begin{equation}
\nabla_{W_1} L = \frac{\partial L}{\partial W_1} \\
\nabla_{W_1} L = \begin{bmatrix}
     \frac{\partial L}{\partial w_{1}^{1}} \\
     \frac{\partial L}{\partial w_{1}^{2}} \\
     \frac{\partial L}{\partial w_{1}^{3}} \\
     \end{bmatrix}
\label{ref20} \tag{10}
\end{equation} 
$$

First, lets compute only \\( \frac{\partial L}{\partial w_{1}^{1}} \\)

$$
\begin{align}
\frac{\partial L}{\partial w_{1}^{1}} & = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial l_1} \times \frac{\partial l_1}{\partial w_1^1} \label{ref21} \tag{11}\\
\frac{\partial L}{\partial \hat{y}} &= \frac{1}{2} \times 2 \times (y - \hat{y})^{1} \times (-1) \label{ref22} \tag{12}\\

\frac{\partial \hat{y}}{\partial l_1} &= \big{(} \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} \times \big{(}1- \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} * -w_2 \\
& = \sigma ([l_1] . [W_2]) \times (1- \sigma ([l_1] . [W_2])) * -w_2 \dots && \text{using \eqref{ref111}} \label{ref23} \tag{13}\\
& = \hat{y} \times (1- \hat{y}) * -w_2 \dots && \text{using \eqref{ref11}} \label{ref24} \tag{14}\\

\frac{\partial l_1}{\partial w_1^1} &= \big{(} \frac{1}{1 + e^{-[X] . [W_1]}} \big{)} \times \big{(}1- \frac{1}{1 + e^{-[X] . [W_1]}} \big{)} * -x_1 \\

\end{align}
$$




#### **+ 2 layer network, 1 training example (vector)**



#### **+ 2 layer network, multiple training examples (vector)**


<!---
Deriving Policy Gradients. I'd like to also give a sketch of where Policy Gradients come from mathematically. Policy Gradients are a special case of a more general score function gradient estimator. The general case is that when we have an expression of the form \(E_{x \sim p(x \mid \theta)} [f(x)] \) - i.e. the expectation of some scalar valued score function \(f(x)\) under some probability distribution \(p(x;\theta)\) parameterized by some \(\theta\). Hint hint, \(f(x)\) will become our reward function (or advantage function more generally) and \(p(x)\) will be our policy network, which is really a model for \(p(a \mid I)\), giving a distribution over actions for any image \(I\). Then we are interested in finding how we should shift the distribution (through its parameters \(\theta\)) to increase the scores of its samples, as judged by \(f\) (i.e. how do we change the network's parameters so that action samples get higher rewards). We have that:

$$
\begin{align}
\nabla_{\theta} E_x[f(x)] &= \nabla_{\theta} \sum_x p(x) f(x) & \text{definition of expectation} \\
& = \sum_x \nabla_{\theta} p(x) f(x) & \text{swap sum and gradient} \\
& = \sum_x p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) & \text{both multiply and divide by } p(x) \\
& = \sum_x p(x) \nabla_{\theta} \log p(x) f(x) & \text{use the fact that } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \\
& = E_x[f(x) \nabla_{\theta} \log p(x) ] & \text{definition of expectation}
\end{align}
$$
-->






    
