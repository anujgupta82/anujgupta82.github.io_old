---
layout: post
comments: true
title:  "Gradients - Part 3"
excerpt: "Part 3 of computing gradients for training Neural Nets"
date:   2016-09-24 15:40:00
mathjax: true
---


### **1 layer network, multiple training examples (each example is a vector)**
Multiple training examples correspond to the scenario of batch training. Our neural net still has 1 layer. 

<div class="imgcap">
<img src="/assets/gradients/NN_2_2.jpeg" height="300" width="350">
<div class="thecap">Neural net with 1 layer. Input is a matrix</div>
</div>

#### **Weights**

W, weight matrix is $$[W]_{\scriptscriptstyle 3 \times 1}$$

$$
\begin{equation}
     W=\begin{bmatrix}
         w_{1} \\
         w_{2} \\
         w_{3} \\
         \end{bmatrix}
\end{equation}
$$

#### **Input & Output definitions**

Input **X** now is a matrix \\( [X]{\scriptscriptstyle 4 \times 3} \\). \\(X_i\\) is the \\(i^{th}\\) sample. So, we now have **4** examples \\(X_1 \ldots X_4\\) , each of which is vector with **3** components. \\(x_{i}^{j}\\) is \\(j^{th}\\) component of \\(X_i\\) sample. So 

$$
\begin{equation}
     X=\begin{bmatrix}
     	x_{1}^{1} & x_{1}^{2} & x_{1}^{3} \\
     	x_{2}^{1} & x_{2}^{2} & x_{2}^{3} \\
     	x_{3}^{1} & x_{3}^{2} & x_{3}^{3} \\
     	x_{4}^{1} & x_{4}^{2} & x_{4}^{3} \\
         \end{bmatrix}
\end{equation}
$$


\\(\vec{y}\\) is a vector. 

$$
\begin{equation}
     y=\begin{bmatrix}
         y_{1} \\
         y_{2} \\
         y_{3} \\
         y_{4} \\
         \end{bmatrix}
\end{equation}
$$

\\(y_i\\) = True label for \\(i^{th}\\) example.

Likewise, \\(\vec{\hat{y}}\\) is a vector where 

\\(\hat{y}_i\\) = predicted label for \\(i^{th}\\) example. Value of \\(\hat{y}_i\\) is computed using \eqref{ref10}:

$$ 
\hat{y}_i = \frac{1}{1+e^{-(x_{i}^{1} \times w_1 + x_{i}^{2} \times w_2 + x_{i}^{3} \times w_3)}} \label{ref10} \tag{0} 
$$

#### **Loss**

Like before, we will use half of squared error loss but in this case it is the total gap in all 4 predictions. Therefore,


$$ L  = \sum\limits_{i=1}^{4} \frac{1}{2} (y_i - \hat{y}_i)^{2} $$

#### **Gradients** 

Let's compute gradients. 

$$
\begin{equation}
\nabla_{W} L = \frac{\partial L}{\partial W} \\
\nabla_{W} L = \begin{bmatrix}
     \frac{\partial L}{\partial w_{1}} \\
     \frac{\partial L}{\partial w_{2}} \\
     \frac{\partial L}{\partial w_{3}} \\
     \end{bmatrix}
\label{ref11} \tag{1}
\end{equation} 
$$

First, lets compute only \\( \frac{\partial L}{\partial w_{1}} \\)

<!--
$$
\begin{align}
\frac{\partial L}{\partial w_1} &= \frac{ }{\partial w_1} (\sum\limits_{i=1}^{4} \frac{1}{2} (y_i - \hat{y}_i)^{2})}{\partial w_1}\\
\end{align}
$$
-->

$$
\begin{align}
\frac{\partial L}{\partial w_1} &= \frac{\partial (\sum\limits_{i=1}^{4} \frac{1}{2} (y_i - \hat{y}_i)^{2})}{\partial w_1} \\
& = \sum\limits_{i=1}^{4} \frac{\partial (\frac{1}{2} (y_i - \hat{y}_i)^{2})}{\partial w_1} \\
& = \sum\limits_{i=1}^{4} \frac{\partial (\frac{1}{2} (y_i - \hat{y}_i)^{2})}{\partial \hat{y}_i} \times \frac{\partial \hat{y}_i}{w_1} \label{ref12} \tag{2}\\
\end{align}
$$

$$
\begin{align}
\frac{\partial (\frac{1}{2} (y_i - \hat{y}_i)^{2})}{\partial \hat{y}_i} = (-1) \times (y_i - \hat{y}_i) \label{ref13} \tag{3}
\end{align}
$$

$$
\begin{align}
\frac{\partial \hat{y}_i}{w_1} &= \sigma(x_{i}^{1} \times w_1 + x_{i}^{2} \times w_2 + x_{i}^{3} \times w_3)(1 - \sigma(x_{i}^{1} \times w_1 + x_{i}^{2} \times w_2 + x_{i}^{3} \times w_3)) \times -x_{i}^{1} \\
& = \sigma( X_i . [W])(1 - \sigma( X_i . [W])) \times (-1) \times x_{i}^{1} \label{ref14} \tag{4}\\
\end{align}
$$

Using \eqref{ref13} and \eqref{ref14} in \eqref{ref12}, we get: 

$$
\begin{align}
\frac{\partial L}{\partial w_1} &= \sum\limits_{i=1}^{4} (y_i - \hat{y}_i) *  [\sigma( X_i . [W])(1 - \sigma( X_i . [W])) \times x_{i}^{1}] \label{ref15} \tag{5}\\
& = (y_1 - \hat{y}_1) * [\sigma( X_1 . [W])(1 - \sigma( X_1 . [W])) \times x_{1}^{1}] + \\
\qquad\qquad	(y_2 - \hat{y}_2) * [\sigma( X_2 . [W])(1 - \sigma(X_2 . [W])) \times x_{2}^{1}] + \\
	(y_3 - \hat{y}_3) * [\sigma( X_3 . [W])(1 - \sigma(X_3 . [W])) \times x_{3}^{1}] + \\
	(y_4 - \hat{y}_4) * [\sigma( X_4 . [W])(1 - \sigma(X_4 . [W])) \times x_{4}^{1}]  \\

\end{align}
$$

Likewise, we get:

$$
\begin{align}
\frac{\partial L}{\partial w_2} &= \sum\limits_{i=1}^{4} (y_i - \hat{y}_i) *  [\sigma( X_i . [W])(1 - \sigma( X_i . [W])) \times x_{i}^{2}] \label{ref16} \tag{6}\\
\end{align}
$$

$$
\begin{align}
\frac{\partial L}{\partial w_3} &= \sum\limits_{i=1}^{4} (y_i - \hat{y}_i) *  [\sigma( X_i . [W])(1 - \sigma( X_i . [W])) \times x_{i}^{3}] \label{ref17} \tag{7}\\
\end{align}
$$

Using \eqref{ref15}, \eqref{ref16} and \eqref{ref17} in \eqref{ref11}, we get: 

$$
\begin{equation}
\frac{\partial L}{\partial W} = \begin{bmatrix}
     \sum\limits_{i=1}^{4} (y_i - \hat{y}_i) *  [\sigma( X_i . [W])(1 - \sigma( X_i . [W])) \times x_{i}^{1}] \\
     \sum\limits_{i=1}^{4} (y_i - \hat{y}_i) *  [\sigma( X_i . [W])(1 - \sigma( X_i . [W])) \times x_{i}^{2}] \\
     \sum\limits_{i=1}^{4} (y_i - \hat{y}_i) *  [\sigma( X_i . [W])(1 - \sigma( X_i . [W])) \times x_{i}^{3}] \\
     \end{bmatrix}
\label{ref18} \tag{8}
\end{equation} 
$$


    
