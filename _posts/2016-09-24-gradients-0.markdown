---
layout: post
comments: true
title:  "Gradients for Neural Nets"
excerpt: "Computing Gradients that go into training Neural Nets"
date:   2016-09-24 15:40:00
mathjax: true
---


### Introduction

Training neural nets is all about [computing gradients](http://deeplearning.stanford.edu/wiki/index.php/Deriving_gradients_using_the_backpropagation_idea). In case you are new to this idea, refer to this awesome [post](http://karpathy.github.io/neuralnets/) by karpathy. In this post we will systmatically see the maths that goes into computing these gradients. The complexity of calculations depends on 3 things: 

1. Depth of the network
2. Number of training examples (1 or more)
3. Number of components in input (1=scalar, >1=vector)

Through out this post we assume:
1. There is no bias term.
2. `.` is matrix multiplication, `*` is element wise product and `X` is normal multiplication. 
3. All activations are [sigmoid a.k.a logistic](https://www.quora.com/What-is-the-sigmoid-function-and-what-is-its-use-in-machine-learnings-neural-networks). It is defined as \\( f(u) = \frac{1}{1+e^{-u}}\\). If you plot it, it comes as:

<div class="imgcap">
<img src="/assets/gradients/logistic.png" height="300" width="350">
<div class="thecap">Sigmoid function</div>
</div>

It easy to see it is smooth and differentiable and bound between 0 and 1. The derivative of the logistic function is simply


We will start with the simplest case and increase the complexity gradually. To keep things simple we will complete it in 5 parts
1. [1 layer network, 1 training example (scalar)]()
2. [1 layer network, 1 training example (vector)]()
3. [1 layer network, batch training (>1 training examples where each is a vector)]()
4. [2 layer network, 1 training example (vector)]()
5. [2 layer network, batch training (>1 training examples where each is a vector)]()

Since we will be dealing with matrices, a key step in every equation is to check if all matrix dimensions are consistent. 

