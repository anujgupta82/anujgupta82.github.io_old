---
layout: post
comments: true
title:  "Gradients for Neural Nets"
excerpt: "Computing Gradients that go into training Neural Nets"
date:   2016-09-24 15:40:00
mathjax: true
---


### Introduction

Training neural nets is all about [computing gradients](http://deeplearning.stanford.edu/wiki/index.php/Deriving_gradients_using_the_backpropagation_idea). In case you are new to this idea, refer to this awesome [post](http://karpathy.github.io/neuralnets/) by karpathy. In this post we will systmatically see the maths that goes into computing these gradients. The complexity of calculations depends on 3 things: 

1. Depth of the network
2. Number of training examples (1 or more)
3. Number of components in input (1=scalar, >1=vector)

Through out this post we assume:
1. There is no bias term.
2. All activations are [sigmoid](https://www.quora.com/What-is-the-sigmoid-function-and-what-is-its-use-in-machine-learnings-neural-networks)
3. `.` is matrix multiplication, `*` is element wise product and `X` is normal multiplication. 


We will start with the simplest case and increase the complexity gradually. To keep things simple we will complete it in 5 parts
1. [1 layer network, 1 training example (scalar)]()
2. [1 layer network, 1 training example (vector)]()
3. [1 layer network, batch training (>1 training examples where each is a vector)]()
4. [2 layer network, 1 training example (vector)]()
5. [2 layer network, batch training (>1 training examples where each is a vector)]()

