---
layout: post
comments: true
title:  "Gradients - Part 2"
excerpt: "Part 2 of computing gradients for training Neural Nets"
date:   2016-08-28 15:40:00
mathjax: true
---



#### **1 layer network, 1 input (vector)**

Our neural net still has 1 layer, but now the input is a vector. 

<div class="imgcap">
<img src="/assets/gradients/NN_2_2.jpeg" height="300" width="350">
<div class="thecap">Neural net with 1 layer, but input is vector</div>
</div>

Input is \\((\vec{X},y)\\) : \\(\vec{X}\\) is a vector, while y is a scalar. 

\\(X = [x^1 ~~x^2 ~~x^3]\\)		&nbsp; &nbsp; &nbsp; \\(x^i = i^{th}\\) component of \\(\vec{X}\\).
	


Thus, in matrix form x,y are $$[X]_{\scriptscriptstyle 1\times 3}$$ and $$[y]_{\scriptscriptstyle 1\times 1}$$. W, weight matrix is $$[W]_{\scriptscriptstyle 3 \times 1}$$

$$
\begin{equation}
     W=\begin{bmatrix}
         w_{1} \\
         w_{2} \\
         w_{3} \\
         \end{bmatrix}
\end{equation}
$$

Let \\( \hat{y} \\) be predicted output. In matrix format, \\([\hat{y}]_{\scriptscriptstyle 1\times 1}\\)

$$
\begin{align}
\hat{y} & = \sigma ([X] . [W]) \label{ref101} \tag{10.1} \\
& = \frac{1}{1 + e^{-[X] . [W]}} \label{ref102} \tag{10.2} \\
& = \frac{1}{1 + e^{-(x^1 w_1 + x^2 w_2 + x^3 w_3)}} \label{ref103} \tag{10.3} \\
\end{align}
$$


Like before, we will use half of squared error loss. $$ L  = \frac{1}{2} (y - \hat{y})^{2} $$

Let's first compute gradients. 

$$
\begin{equation}
\nabla_{W} L = \frac{\partial L}{\partial W} \\
\nabla_{W} L = \begin{bmatrix}
     \frac{\partial L}{\partial w_{1}} \\
     \frac{\partial L}{\partial w_{2}} \\
     \frac{\partial L}{\partial w_{3}} \\
     \end{bmatrix}
\label{ref11} \tag{11}
\end{equation}
$$


So, lets compute \\( \frac{\partial L}{\partial w_{1}} \\)

$$
\begin{align}
\frac{\partial L}{\partial w_1} &= \frac{\partial L}{\partial \hat{y}} * \frac{\partial \hat{y}}{\partial w_1} \label{ref12} \tag{12} \\
\frac{\partial L}{\partial \hat{y}} &= \frac{1}{2} \times 2 \times (y - \hat{y})^{1} \times (-1) \label{ref13} \tag{13} \\
\frac{\partial \hat{y}}{\partial w_1} &= \big{(} \frac{1}{1 + e^{-[X] . [W]}} \big{)} \times \big{(}1- \frac{1}{1 + e^{-[X] . [W]}} \big{)} \times x_1 \dots & \text{using \eqref{ref102} & \eqref{ref103}} \label{ref14} \tag{14}\\
& = \sigma ([X] . [W]) \times (1- \sigma ([X] . [W])) * x_1 \dots & \text{using \eqref{ref101}} & \label{ref15} \tag{15}\\
& = \hat{y} \times (1- \hat{y}) \times x_1 \dots & \text{using \eqref{ref101}} & \label{ref16} \tag{16}\\
\end{align}
$$

Substituting \eqref{ref13} & \eqref{ref16} in \eqref{ref12}, we get 

$$
\begin{align}
\frac{\partial L}{\partial w_1} &= \big{(} (-1) \times (y - \hat{y}) \big{)} \times \big{(} \hat{y} \times (1- \hat{y}) \times x_1 \big{)} \\
& = -(y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \times x_1\\
& = (\hat{y} - y) \times \hat{y} \times (1- \hat{y}) \times x_1
\end{align}
$$

Thus, in general:
$$
\begin{align}
\frac{\partial L}{\partial w_i} &= (\hat{y} - y) \times \hat{y} \times (1- \hat{y}) \times x_i \label{ref17} \tag{17}\\
\end{align}
$$

Using \eqref{ref17} in \eqref{ref11}


$$
\begin{equation}
\frac{\partial L}{\partial W} = \begin{bmatrix}
     (\hat{y} - y) \times \hat{y} \times (1- \hat{y}) \times x_1 \\
     (\hat{y} - y) \times \hat{y} \times (1- \hat{y}) \times x_2 \\
     (\hat{y} - y) \times \hat{y} \times (1- \hat{y}) \times x_3 \\
     \end{bmatrix}
\label{ref18} \tag{18}
\end{equation}
$$


$$
\begin{equation}
= \begin{bmatrix}
     x^1 \\
     x^2 \\
     x^3 \\
     \end{bmatrix} 
     * [(\hat{y} - y) \times \hat{y} \times (1- \hat{y})]
\label{ref19} \tag{19}
\end{equation}
$$

<!--
$$
\begin{equation}
= [x^1 ~~x^2 ~~x^3] .
	\begin{bmatrix}
     (y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \\
     (y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \\
     (y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \\
     \end{bmatrix}
\label{ref19} \tag{19}
\end{equation}
$$
-->

Let,  

\begin{align}
\Delta l_{1} = (\hat{y} - y) \times \hat{y} \times (1- \hat{y}) \label{ref20} \tag{20} \\
\end{align}

Using \eqref{ref20} in \eqref{ref19}

$$ 
      \begin{align}
      \frac{\partial L}{\partial W} &= [X^{T}] . \Delta l_{1} \\
      \end{align}
$$


[__Prev__](https://anujgupta82.github.io/2016/08/26/gradients-1/) [__Next__](https://anujgupta82.github.io/2016/08/30/gradients-3/)
