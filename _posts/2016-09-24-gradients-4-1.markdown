---
layout: post
comments: true
title:  "Gradients - Part 4a"
excerpt: "Part 4 of computing gradients for training Neural Nets"
date:   2016-09-24 15:40:00
mathjax: true
---


### **2 layer network, single training example (vector)**

In this post we will consider 2 type of networks. In first network, hidden layer has only 1 node while in second network hideen layer has more than 1 node. Lets start with the case of hidden layer with only 1 node.

#### **1 hidden layer with 1 node**


<div class="imgcap">
<img src="/assets/gradients/NN_4_1.jpeg" height="300" width="350">
<div class="thecap">Neural net with 2 layer, 1 node in hidden layer. Input is a vector</div>
</div>

First layer (\\(l_1\\)) has weight matrix $$[W_1]_{\scriptscriptstyle 3 \times 1}$$
$$
\begin{equation}
     W_1=\begin{bmatrix}
         w_{1}^{1} \\
         w_{1}^{2} \\
         w_{1}^{3} \\
         \end{bmatrix}
\end{equation}
$$

Second layer (\\(l_2\\)) has weight matrix $$[W_2]_{\scriptscriptstyle 1 \times 1}$$ which is a scalar 

$$
\begin{equation}
     W_2=\begin{bmatrix}
         w_{2} \\
         \end{bmatrix}
\end{equation}
$$

##### **Input & Output definitions**

Input is \\((\vec{X},y)\\) : \\(\vec{X}\\) is a vector, while y is a scalar. 

\\(X = [x^1 ~~x^2 ~~x^3]\\)     &nbsp; &nbsp; &nbsp; \\(x^i = i^{th}\\) component of \\(\vec{X}\\).

Thus, in matrix form x,y are $$[X]_{\scriptscriptstyle 1\times 3}$$ and $$[y]_{\scriptscriptstyle 1\times 1}$$.

Let \\( l_1 \\) be output of layer 1. In matrix format, \\([l_1]_{\scriptscriptstyle 1\times 1}\\)

$$
\begin{align}
l_1 & = \sigma ([X] . [W_1]) \label{ref0} \tag{1.1} \\
& = \frac{1}{1 + e^{-[X] . [W_1]}} \label{ref10} \tag{1.2} \\
& = \frac{1}{1 + e^{-(x^1 \times w_1^1 + x^2 \times w_1^2 + x^3 \times w_1^3)}} \label{ref101} \tag{1.3} 
\end{align}
$$


\\( \hat{y} \\) is predicted output. In matrix format, \\([\hat{y}]_{\scriptscriptstyle 1\times 1}\\)

$$
\begin{align}
\hat{y} & = \sigma ([l_1] . [W_2]) \label{ref11} \tag{2.1} \\
& = \frac{1}{1 + e^{-[l_1] . [W_2]}} \label{ref112} \tag{2.2} \\
& = \frac{1}{1 + e^{-(l_1^1 w_2^1 + l_1^2 w_2^2)}} \label{ref113} \tag{2.3} \\
\end{align}
$$


##### **Loss**
Like before, we will use half of squared error loss. $$ L  = \frac{1}{2} (y - \hat{y})^{2} $$



##### **Gradients**

There are 2 set of gradients: \\(\nabla_{W_1} L\\) and \\(\nabla_{W_2} L\\). Let us first compute \\(\nabla_{W_2} L\\)


**\\([\nabla_{W_2} L]_{\scriptscriptstyle 1\times 1}\\)**

$$
\begin{align}
\frac{\partial L}{\partial W_2} & = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial W_2} \label{ref12} \tag{3}\\
\frac{\partial L}{\partial \hat{y}} &= \frac{1}{2} \times 2 \times (y - \hat{y})^{1} \times (-1) \label{ref13} \tag{4}\\
\frac{\partial \hat{y}}{\partial W_2} &= \big{(} \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} \times \big{(}1- \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} * -l_1 \\
& = \sigma ([l_1] . [W_2]) \times (1- \sigma ([l_1] . [W_2])) * -l_1 \dots && \text{using \eqref{ref11}} \label{ref14} \tag{5}\\
& = \hat{y} \times (1- \hat{y}) * -l_1 \dots && \text{using \eqref{ref11}} \label{ref15} \tag{6}\\
\end{align}
$$

Substituting \eqref{ref13} & \eqref{ref15} in \eqref{ref12}, we get 

$$
\begin{align}
\frac{\partial L}{\partial W_2} &= \big{(} (-1) \times (y - \hat{y}) \big{)} \times \big{(} \hat{y} \times (1- \hat{y}) \times -l_1 \big{)}\\ \\
&= (y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \times l_1 \label{ref16} \tag{7} \\
\end{align}
$$

Let,  

\begin{align}
\Delta l_{2} = (y - \hat{y}) \times \hat{y} \times (1- \hat{y}) \label{ref17} \tag{8} \\
\end{align}

Then, eq \eqref{ref16} reduces to:   
$$ 
\begin{align}
\frac{\partial L}{\partial W_2} &= \Delta l_{2} \times l_1 \\
& = \Delta l_{2} * l_1 \\
& = [l_1]^\intercal . \Delta l_{2} \label{ref18} \tag{9} \\
\end{align}
$$

**\\([\nabla_{W_1} L]_{\scriptscriptstyle 3\times 1}\\)**

Now let us compute \\(\nabla_{W_1} L\\)

$$
\begin{equation}
\nabla_{W_1} L = \frac{\partial L}{\partial W_1} \\
\nabla_{W_1} L = \begin{bmatrix}
     \frac{\partial L}{\partial w_{1}^{1}} \\
     \frac{\partial L}{\partial w_{1}^{2}} \\
     \frac{\partial L}{\partial w_{1}^{3}} \\
     \end{bmatrix}
\label{ref20} \tag{10}
\end{equation} 
$$

First, lets compute only \\( \frac{\partial L}{\partial w_{1}^{1}} \\)

$$
\begin{align}
\frac{\partial L}{\partial w_{1}^{1}} & = \frac{\partial L}{\partial \hat{y}} \times \frac{\partial \hat{y}}{\partial l_1} \times \frac{\partial l_1}{\partial w_1^1} \label{ref21} \tag{11}\\
\frac{\partial L}{\partial \hat{y}} &= \frac{1}{2} \times 2 \times (y - \hat{y})^{1} \times (-1) \label{ref22} \tag{12}\\

\frac{\partial \hat{y}}{\partial l_1} &= \big{(} \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} \times \big{(}1- \frac{1}{1 + e^{-[l_1] . [W_2]}} \big{)} * -w_2 \\
& = \sigma ([l_1] . [W_2]) \times (1- \sigma ([l_1] . [W_2])) * -w_2 \dots && \text{using \eqref{ref11}} \label{ref23} \tag{13}\\
& = \hat{y} \times (1- \hat{y}) * -w_2 \dots && \text{using \eqref{ref11}} \label{ref24} \tag{14}\\

\frac{\partial l_1}{\partial w_1^1} &= \big{(} \frac{1}{1 + e^{-[X] . [W_1]}} \big{)} \times \big{(}1- \frac{1}{1 + e^{-[X] . [W_1]}} \big{)} * -x^1 \label{ref25} \tag{15}\\
& = l_1 \times (1- l_1) * -x^1 \dots && \text{using \eqref{ref11}} \label{ref26} \tag{16}\\

\end{align}
$$

Combining, eq \eqref{ref22}, \eqref{ref24} and \eqref{ref26}:  

$$
\begin{align}
\frac{\partial L}{\partial w_{1}^{1}} & = \big(-(y - \hat{y}) \big) \times \big( \hat{y} \times (1- \hat{y}) * -w_2 \big) \times  \big( l_1 \times (1- l_1) * -x^1 \big) \\
& = \big((y - \hat{y})\hat{y}(1- \hat{y}) * w_2 \big) \times  \big( l_1 \times (1- l_1) * -x^1 \big) \\

& = \big(- \Delta l_{2} \times  w_2 \times l_1 \times (1- l_1) \big) \times x^1  \dots && \text{using \eqref{ref17}} \label{ref27} \tag{17}\\

\end{align}
$$

Let,  

\begin{align}
\Delta l_{1} = - \Delta l_{2} \times  w_2 \times l_1 \times (1- l_1) \label{ref28} \tag{18} \\
\end{align}

Then, eq \eqref{ref27} reduces to :

$$
\begin{align}
\frac{\partial L}{\partial w_{1}^{1}} & = \Delta l_{1} \times x^1 \label{ref29} \tag{19} \\
\end{align}
$$

Likewise, 
$$
\begin{align}
\frac{\partial L}{\partial w_{1}^{2}} & = \Delta l_{1} \times x^2 \label{ref30} \tag{20} \\
\frac{\partial L}{\partial w_{1}^{3}} & = \Delta l_{1} \times x^3 \label{ref31} \tag{21} \\
\end{align}
$$

Using eq \eqref{ref29}, \eqref{ref30}  and \eqref{ref31}  in \eqref{ref20} 

$$
\begin{equation}
\nabla_{W_1} L = \begin{bmatrix}
     \Delta l_{1} \times x^1 \\
     \Delta l_{1} \times x^2 \\
     \Delta l_{1} \times x^3 \\
     \end{bmatrix}
\label{ref32} \tag{22}
\end{equation} 
$$

$$
\frac{\partial L}{\partial W_1} = \left[ \begin{array}{c} x^1 \\ x^2 \\ x^3 \end{array} \right]_{\scriptscriptstyle 3 \times 1}. \left[ \Delta l_{1} \right]_{\scriptscriptstyle 1 \times 1}
$$

$$ 
\begin{align}
\frac{\partial L}{\partial W_1} &= [X]^\intercal . \Delta l_{1} \\
\end{align}
$$

In the [next post](https://anujgupta82.github.io/2016/09/24/gradients-4-2/) we will see what happens when the hidden layer has more than 1 nodes.